{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7741c00-deb1-427c-8175-ed0e38a3c8f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_81024/2461910642.py:4: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from typing import Tuple\n",
    "import pandas\n",
    "\n",
    "sys.path.append('/home/danillorp/√Årea de Trabalho/github/fema/src/')\n",
    "\n",
    "\n",
    "import fema_semi\n",
    "import fema_classifier\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a4a3157-c950-442e-bcf3-a3ca1012c940",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The experiment will run for  16\n"
     ]
    }
   ],
   "source": [
    "ml_datasets = ['satellite','one-hundred-plants-texture',\n",
    "               'steel-plates-fault','arrhythmia','Titanic','kr-vs-kp','bank-marketing','vehicle','adult','airlines','PhishingWebsites',\n",
    "               'kc1','pc1','cmc','mfeat-factors','KDDCup09_appetency']\n",
    "\n",
    "#'credit-g','SpeedDating','blood-transfusion-service-center','mnist_784',\n",
    "print('The experiment will run for ',len(ml_datasets))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "336b399b-cb34-433b-8c99-f510328f8728",
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment(dataset_name:str, data_x:pandas.core.frame.DataFrame, data_y:pandas.core.series.Series, test_size:float):\n",
    "    train_x, test_x, train_y, test_y = train_test_split(data_x, data_y, test_size=test_size)\n",
    "    uknw_x, test_x, uknw_y, test_y = train_test_split(test_x, test_y, test_size=(1-test_size))\n",
    "\n",
    "    print(dataset_name)\n",
    "    print('train_size',len(train_x),'classes:',set(train_y))\n",
    "    print('test_size',len(test_x),'classes:',set(test_y))\n",
    "    print('uknw_size',len(uknw_x),'classes:',set(uknw_y))\n",
    "\n",
    "    #train_y = train_y.to_numpy()\n",
    "    #test_y = test_y.to_numpy()\n",
    "    #uknw_y = uknw_y.to_numpy()\n",
    "\n",
    "    train_y = train_y[:,np.newaxis]\n",
    "    test_y = test_y[:,np.newaxis]\n",
    "    uknw_y = uknw_y[:,np.newaxis]\n",
    "\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    train_x = scaler.fit_transform(train_x)\n",
    "    test_x = scaler.transform(test_x)\n",
    "    uknw_x = scaler.transform(uknw_x)\n",
    "\n",
    "    #print(type(train_y),train_y.shape,train_y[:,0])\n",
    "\n",
    "    model_fema_original = fema_classifier.FEMaClassifier(k=10,basis=fema_classifier.Basis.shepardBasis)\n",
    "    model_fema_original.fit(train_x,train_y)\n",
    "\n",
    "    pred, confidence_level = model_fema_original.predict(test_x,2)\n",
    "\n",
    "    cm_fema_original = confusion_matrix(test_y,pred)\n",
    "    bal_acc_original = balanced_accuracy_score(test_y, pred)\n",
    "    acc_original = accuracy_score(test_y, pred)\n",
    "\n",
    "    print(dataset_name,cm_fema_original, bal_acc_original, acc_original)\n",
    "\n",
    "    model_semi = fema_semi.FEMaSemiSupervisedClassifier(k=10,basis=fema_semi.Basis.shepardBasis)\n",
    "    model_semi.fit(train_x,train_y,uknw_x,2)\n",
    "    \n",
    "    pred, confidence_level = model_semi.predict(test_x,2)\n",
    "    \n",
    "    cm_fema_semi = confusion_matrix(test_y,pred)\n",
    "    bal_acc_semi = balanced_accuracy_score(test_y, pred)\n",
    "    acc_semi = accuracy_score(test_y, pred)\n",
    "    \n",
    "    \n",
    "    print(cm_fema_semi, bal_acc_semi, acc_semi)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c33632-6080-4e00-9c9d-cc1103ad2d43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "satellite (5100, 36) (5100,) <class 'pandas.core.frame.DataFrame'> <class 'numpy.ndarray'>\n",
      "V1     int64\n",
      "V2     int64\n",
      "V3     int64\n",
      "V4     int64\n",
      "V5     int64\n",
      "V6     int64\n",
      "V7     int64\n",
      "V8     int64\n",
      "V9     int64\n",
      "V10    int64\n",
      "V11    int64\n",
      "V12    int64\n",
      "V13    int64\n",
      "V14    int64\n",
      "V15    int64\n",
      "V16    int64\n",
      "V17    int64\n",
      "V18    int64\n",
      "V19    int64\n",
      "V20    int64\n",
      "V21    int64\n",
      "V22    int64\n",
      "V23    int64\n",
      "V24    int64\n",
      "V25    int64\n",
      "V26    int64\n",
      "V27    int64\n",
      "V28    int64\n",
      "V29    int64\n",
      "V30    int64\n",
      "V31    int64\n",
      "V32    int64\n",
      "V33    int64\n",
      "V34    int64\n",
      "V35    int64\n",
      "V36    int64\n",
      "dtype: object\n",
      "satellite\n",
      "train_size 4080 classes: {0, 1}\n",
      "test_size 816 classes: {0, 1}\n",
      "uknw_size 204 classes: {1}\n",
      "satellite [[  3   7]\n",
      " [  0 806]] 0.65 0.991421568627451\n",
      "204 2\n",
      "[[  3   7]\n",
      " [  0 806]] 0.65 0.991421568627451\n",
      "one-hundred-plants-texture (1599, 64) (1599,) <class 'pandas.core.frame.DataFrame'> <class 'numpy.ndarray'>\n",
      "V1     float64\n",
      "V2     float64\n",
      "V3     float64\n",
      "V4     float64\n",
      "V5     float64\n",
      "        ...   \n",
      "V60    float64\n",
      "V61    float64\n",
      "V62    float64\n",
      "V63    float64\n",
      "V64    float64\n",
      "Length: 64, dtype: object\n",
      "one-hundred-plants-texture\n",
      "train_size 1279 classes: {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99}\n",
      "test_size 256 classes: {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 40, 41, 42, 43, 44, 45, 46, 47, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99}\n",
      "uknw_size 64 classes: {2, 5, 6, 8, 9, 10, 11, 12, 13, 15, 16, 19, 20, 21, 23, 24, 26, 27, 28, 30, 32, 33, 34, 35, 36, 46, 47, 48, 49, 53, 54, 57, 62, 63, 65, 68, 69, 71, 73, 75, 83, 84, 87, 89, 90, 91, 95}\n"
     ]
    }
   ],
   "source": [
    "for dataset in ml_datasets: \n",
    "    data = fetch_openml(dataset,version=1)\n",
    "    X, y = data['data'], data['target']\n",
    "\n",
    "    X = X.select_dtypes(include=np.number)\n",
    "\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    le.fit(y)\n",
    "    y = le.transform(y)\n",
    "    \n",
    "    print(dataset, X.shape, y.shape, type(X), type(y))\n",
    "    print(X.dtypes)\n",
    "    experiment(dataset_name=dataset, data_x=X, data_y=y, test_size=0.20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
