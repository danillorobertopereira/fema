{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering, SpectralClustering\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score, adjusted_rand_score, normalized_mutual_info_score\n",
    "from sklearn.datasets import fetch_openml, fetch_20newsgroups\n",
    "from torchvision.datasets import CIFAR10, CIFAR100\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "\n",
    "import sys\n",
    "import os\n",
    "from typing import Tuple\n",
    "\n",
    "sys.path.append('/home/danillorp/Área de Trabalho/github/fema/src/')\n",
    "\n",
    "from fema_clustering import FEMaClustering\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Função para carregar e preparar os datasets\n",
    "\n",
    "def load_datasets():\n",
    "    # Carregar datasets do sklearn\n",
    "    print('Loading dataset Iris ...')\n",
    "    iris = datasets.load_iris()\n",
    "    print('Loading dataset Wine ...')\n",
    "    wine = datasets.load_wine()\n",
    "    print('Loading dataset Digits ...')\n",
    "    digits = datasets.load_digits()\n",
    "    \n",
    "    # Carregar MNIST\n",
    "    \n",
    "    \"\"\"print('Loading dataset MNIST ...')\n",
    "    mnist = fetch_openml('mnist_784', version=1)\n",
    "    mnist_data = mnist.data\n",
    "    mnist_target = mnist.target.astype(int)\n",
    "    \n",
    "    # Carregar KDD Cup 99\n",
    "    print('Loading dataset KDDCup99 ...')\n",
    "    kddcup = fetch_openml('KDDCup99', version=1)\n",
    "    kddcup_data = kddcup.data\n",
    "    kddcup_target = kddcup.target\n",
    "\n",
    "    # Carregar CIFAR-10\n",
    "    print('Loading dataset CIFAR-10 ...')\n",
    "    transform = transforms.Compose([transforms.ToTensor(), transforms.Lambda(lambda x: x.view(-1))])\n",
    "    cifar10 = CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "    cifar10_data = torch.stack([data for data, _ in DataLoader(cifar10, batch_size=len(cifar10))]).squeeze()\n",
    "    cifar10_target = np.array(cifar10.targets)\n",
    "    \n",
    "    # Carregar CIFAR-100\n",
    "    print('Loading dataset CIFAR-100 ...')\n",
    "    cifar100 = CIFAR100(root='./data', train=True, download=True, transform=transform)\n",
    "    cifar100_data = torch.stack([data for data, _ in DataLoader(cifar100, batch_size=len(cifar100))]).squeeze()\n",
    "    cifar100_target = np.array(cifar100.targets)\n",
    "\n",
    "    # Carregar 20 Newsgroups\n",
    "    print('Loading dataset 20newsgroups ...')\n",
    "    newsgroups = fetch_20newsgroups(subset='all')\n",
    "    newsgroups_data = newsgroups.data\n",
    "    newsgroups_target = newsgroups.target\n",
    "\n",
    "    return {\n",
    "        'Iris': (iris.data, iris.target),\n",
    "        'Wine': (wine.data, wine.target),\n",
    "        'Digits': (digits.data, digits.target),\n",
    "        'MNIST': (mnist_data, mnist_target),\n",
    "        'KDD Cup 99': (kddcup_data, kddcup_target),\n",
    "        'CIFAR-10': (cifar10_data, cifar10_target),\n",
    "        'CIFAR-100': (cifar100_data, cifar100_target),\n",
    "        '20 Newsgroups': (newsgroups_data, newsgroups_target)\n",
    "    }\n",
    "    \n",
    "    \"\"\"\n",
    "    return {\n",
    "        'Iris': (iris.data, iris.target),\n",
    "        'Wine': (wine.data, wine.target),\n",
    "        'Digits': (digits.data, digits.target),\n",
    "     }\n",
    "     \n",
    "# Função para normalizar e reduzir dimensionalidade\n",
    "def preprocess_data(data):\n",
    "    scaler = StandardScaler()\n",
    "    data_scaled = scaler.fit_transform(data)\n",
    "    pca = PCA(n_components=2)\n",
    "    data_reduced = pca.fit_transform(data_scaled)\n",
    "    return data_reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Função para aplicar métodos de clusterização\n",
    "def apply_clustering_methods(data):\n",
    "    clustering_methods = {\n",
    "        'KMeans': KMeans(n_clusters=3, random_state=42),\n",
    "        'DBSCAN': DBSCAN(eps=0.5, min_samples=5),\n",
    "        'Agglomerative': AgglomerativeClustering(n_clusters=3),\n",
    "        'GMM': GaussianMixture(n_components=3, random_state=42),\n",
    "        'Spectral': SpectralClustering(n_clusters=3, affinity='nearest_neighbors', random_state=42),\n",
    "        'FEMaClustering': FEMaClustering(z=2)\n",
    "    }\n",
    "    \n",
    "    clusters = {}\n",
    "    for method_name, method in clustering_methods.items():\n",
    "        if method_name == 'GMM':\n",
    "            method.fit(data)\n",
    "            labels = method.predict(data)\n",
    "        elif method_name == 'FEMaClustering':\n",
    "            print('FEMaClustering... ...')\n",
    "            method.fit(data,min_distance=0.1,qtd_samples_perc=0.25)\n",
    "            labels = method.predict(th_same_cluster=0.9,qtd_diff_samples=50)\n",
    "        else:\n",
    "            method.fit(data)\n",
    "            labels = method.labels_\n",
    "        clusters[method_name] = labels\n",
    "        \n",
    "    return clusters\n",
    "\n",
    "# Função para calcular as métricas\n",
    "def calculate_metrics(data, labels_true, labels_pred):\n",
    "    metrics = {\n",
    "        'Silhouette Score': silhouette_score(data, labels_pred),\n",
    "        'Davies-Bouldin Score': davies_bouldin_score(data, labels_pred),\n",
    "        'Adjusted Rand Index': adjusted_rand_score(labels_true, labels_pred),\n",
    "        'Normalized Mutual Information': normalized_mutual_info_score(labels_true, labels_pred)\n",
    "    }\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset Iris ...\n",
      "Loading dataset Wine ...\n",
      "Loading dataset Digits ...\n",
      "\n",
      "Repetition 1/10\n",
      "\n",
      "Processing Iris dataset\n",
      "INFO: (150, 4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/danillorp/.local/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FEMaClustering... ...\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Supondo que as funções load_datasets, preprocess_data, apply_clustering_methods e calculate_metrics já existam\n",
    "\n",
    "preprocess_data_flag = True\n",
    "N = 10  # Número de repetições\n",
    "\n",
    "# Função principal para executar o experimento\n",
    "def main():\n",
    "    datasets = load_datasets()\n",
    "    \n",
    "    all_results = []\n",
    "\n",
    "    for repetition in range(N):\n",
    "        print(f\"\\nRepetition {repetition + 1}/{N}\")\n",
    "\n",
    "        for dataset_name, (data, target) in datasets.items():\n",
    "            print(f\"\\nProcessing {dataset_name} dataset\")\n",
    "            print('INFO:', data.shape)\n",
    "            \n",
    "            if dataset_name in ['20 Newsgroups']:\n",
    "                # Para o conjunto de dados 20 Newsgroups, a vetorização do texto é necessária\n",
    "                vectorizer = TfidfVectorizer(max_features=1000)\n",
    "                data = vectorizer.fit_transform(data).toarray()\n",
    "            \n",
    "            if preprocess_data_flag:\n",
    "                data_preprocessed = preprocess_data(data)\n",
    "            else:\n",
    "                data_preprocessed = data.copy()\n",
    "\n",
    "            clusters = apply_clustering_methods(data_preprocessed)\n",
    "            \n",
    "            for method_name, labels_pred in clusters.items():\n",
    "                metrics = calculate_metrics(data_preprocessed, target, labels_pred)\n",
    "                \n",
    "                # Adiciona os resultados para esta repetição e método na lista de resultados\n",
    "                result_row = {\n",
    "                    'Repetition': repetition + 1,\n",
    "                    'Dataset': dataset_name,\n",
    "                    'Method': method_name\n",
    "                }\n",
    "                result_row.update(metrics)\n",
    "                all_results.append(result_row)\n",
    "\n",
    "                print(f\"\\nResults for {method_name} on {dataset_name}:\")\n",
    "                for metric_name, metric_value in metrics.items():\n",
    "                    print(f\"{metric_name}: {metric_value:.4f}\")\n",
    "\n",
    "                if preprocess_data_flag:\n",
    "                    plt.figure()  # Cria uma nova figura para cada método\n",
    "                    plt.scatter(data_preprocessed[:, 0], data_preprocessed[:, 1], c=labels_pred)  # Plotar dados com cores de cluster\n",
    "                    plt.title(f\"{method_name} on {dataset_name}\")\n",
    "                    plt.xlabel(\"Component 1\")\n",
    "                    plt.ylabel(\"Component 2\")\n",
    "                    plt.savefig(f\"figs/{dataset_name}_{method_name}_rep{repetition + 1}.png\")\n",
    "\n",
    "    # Salva todos os resultados em um arquivo CSV\n",
    "    results_df = pd.DataFrame(all_results)\n",
    "    results_df.to_csv('clustering_results.csv', index=False)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "\n",
    "\"\"\"Para o Silhouette Score e Davies-Bouldin Score, quanto maior o valor, melhor é o agrupamento.\n",
    "Para o ARI e NMI, quanto mais próximo de 1, melhor é a concordância entre os agrupamentos e os rótulos verdadeiros.\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
