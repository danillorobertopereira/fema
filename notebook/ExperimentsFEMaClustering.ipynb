{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering, SpectralClustering\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score, adjusted_rand_score, normalized_mutual_info_score\n",
    "from sklearn.datasets import fetch_openml, fetch_20newsgroups\n",
    "from torchvision.datasets import CIFAR10, CIFAR100\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "import sys\n",
    "import os\n",
    "from typing import Tuple\n",
    "\n",
    "sys.path.append('/home/danillorp/Área de Trabalho/github/fema/src/')\n",
    "\n",
    "from fema_clustering import FEMaClustering\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Função para carregar e preparar os datasets\n",
    "\n",
    "def load_datasets():\n",
    "    # Carregar datasets do sklearn\n",
    "    print('Loading dataset Iris ...')\n",
    "    iris = datasets.load_iris()\n",
    "    print('Loading dataset Wine ...')\n",
    "    wine = datasets.load_wine()\n",
    "    print('Loading dataset Digits ...')\n",
    "    digits = datasets.load_digits()\n",
    "    \n",
    "    # Carregar MNIST\n",
    "    \n",
    "    print('Loading dataset MNIST ...')\n",
    "    mnist = fetch_openml('mnist_784', version=1)\n",
    "    mnist_data = mnist.data\n",
    "    mnist_target = mnist.target.astype(int)\n",
    "    \n",
    "    # Carregar KDD Cup 99\n",
    "    print('Loading dataset KDDCup99 ...')\n",
    "    kddcup = fetch_openml('KDDCup99', version=1)\n",
    "    kddcup_data = kddcup.data\n",
    "    kddcup_target = kddcup.target\n",
    "\n",
    "    # Carregar CIFAR-10\n",
    "    print('Loading dataset CIFAR-10 ...')\n",
    "    transform = transforms.Compose([transforms.ToTensor(), transforms.Lambda(lambda x: x.view(-1))])\n",
    "    cifar10 = CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "    cifar10_data = torch.stack([data for data, _ in DataLoader(cifar10, batch_size=len(cifar10))]).squeeze()\n",
    "    cifar10_target = np.array(cifar10.targets)\n",
    "    \n",
    "    # Carregar CIFAR-100\n",
    "    print('Loading dataset CIFAR-100 ...')\n",
    "    cifar100 = CIFAR100(root='./data', train=True, download=True, transform=transform)\n",
    "    cifar100_data = torch.stack([data for data, _ in DataLoader(cifar100, batch_size=len(cifar100))]).squeeze()\n",
    "    cifar100_target = np.array(cifar100.targets)\n",
    "\n",
    "    # Carregar 20 Newsgroups\n",
    "    print('Loading dataset 20newsgroups ...')\n",
    "    newsgroups = fetch_20newsgroups(subset='all')\n",
    "    newsgroups_data = newsgroups.data\n",
    "    newsgroups_target = newsgroups.target\n",
    "\n",
    "    return {\n",
    "        'Iris': (iris.data, iris.target),\n",
    "        'Wine': (wine.data, wine.target),\n",
    "        'Digits': (digits.data, digits.target),\n",
    "        'MNIST': (mnist_data, mnist_target),\n",
    "        'KDD Cup 99': (kddcup_data, kddcup_target),\n",
    "        'CIFAR-10': (cifar10_data, cifar10_target),\n",
    "        'CIFAR-100': (cifar100_data, cifar100_target),\n",
    "        '20 Newsgroups': (newsgroups_data, newsgroups_target)\n",
    "    }\n",
    "    \n",
    "    \"\"\"return {\n",
    "        'Iris': (iris.data, iris.target),\n",
    "        'Wine': (wine.data, wine.target),\n",
    "        'Digits': (digits.data, digits.target),\n",
    "     }\n",
    "     \"\"\"\n",
    "\n",
    "# Função para normalizar e reduzir dimensionalidade\n",
    "def preprocess_data(data):\n",
    "    scaler = StandardScaler()\n",
    "    data_scaled = scaler.fit_transform(data)\n",
    "    pca = PCA(n_components=2)\n",
    "    data_reduced = pca.fit_transform(data_scaled)\n",
    "    return data_reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Função para aplicar métodos de clusterização\n",
    "def apply_clustering_methods(data):\n",
    "    clustering_methods = {\n",
    "        'KMeans': KMeans(n_clusters=3, random_state=42),\n",
    "        'DBSCAN': DBSCAN(eps=0.5, min_samples=5),\n",
    "        'Agglomerative': AgglomerativeClustering(n_clusters=3),\n",
    "        'GMM': GaussianMixture(n_components=3, random_state=42),\n",
    "        'Spectral': SpectralClustering(n_clusters=3, affinity='nearest_neighbors', random_state=42),\n",
    "        'FEMaClustering': FEMaClustering(z=2)\n",
    "    }\n",
    "    \n",
    "    clusters = {}\n",
    "    for method_name, method in clustering_methods.items():\n",
    "        if method_name == 'GMM':\n",
    "            method.fit(data)\n",
    "            labels = method.predict(data)\n",
    "        elif method_name == 'FEMaClustering':\n",
    "            print('FEMaClustering... ...')\n",
    "            method.fit(data,min_distance=0.1,qtd_samples_perc=0.25)\n",
    "            labels = method.predict(th_same_cluster=0.9,qtd_diff_samples=50)\n",
    "        else:\n",
    "            method.fit(data)\n",
    "            labels = method.labels_\n",
    "        clusters[method_name] = labels\n",
    "        \n",
    "    return clusters\n",
    "\n",
    "# Função para calcular as métricas\n",
    "def calculate_metrics(data, labels_true, labels_pred):\n",
    "    metrics = {\n",
    "        'Silhouette Score': silhouette_score(data, labels_pred),\n",
    "        'Davies-Bouldin Score': davies_bouldin_score(data, labels_pred),\n",
    "        'Adjusted Rand Index': adjusted_rand_score(labels_true, labels_pred),\n",
    "        'Normalized Mutual Information': normalized_mutual_info_score(labels_true, labels_pred)\n",
    "    }\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "preprocess_data_flag = True\n",
    "\n",
    "# Função principal para executar o experimento\n",
    "def main():\n",
    "    datasets = load_datasets()\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for dataset_name, (data, target) in datasets.items():\n",
    "        print(f\"\\nProcessing {dataset_name} dataset\")\n",
    "        print('INFO:',data.shape)\n",
    "        \n",
    "        if dataset_name in ['20 Newsgroups']:\n",
    "            # Para o conjunto de dados 20 Newsgroups, a vetorização do texto é necessária\n",
    "            from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "            vectorizer = TfidfVectorizer(max_features=1000)\n",
    "            data = vectorizer.fit_transform(data).toarray()\n",
    "        \n",
    "        if preprocess_data_flag:\n",
    "            data_preprocessed = preprocess_data(data)\n",
    "        else:   \n",
    "            data_preprocessed = data.copy()\n",
    "\n",
    "        clusters = apply_clustering_methods(data_preprocessed)\n",
    "        \n",
    "        for method_name, labels_pred in clusters.items():\n",
    "            metrics = calculate_metrics(data_preprocessed, target, labels_pred)\n",
    "            results[(dataset_name, method_name)] = metrics\n",
    "            \n",
    "\n",
    "            print(f\"\\nResults for {method_name} on {dataset_name}:\")\n",
    "            for metric_name, metric_value in metrics.items():\n",
    "                print(f\"{metric_name}: {metric_value:.4f}\")\n",
    "\n",
    "            if preprocess_data_flag:\n",
    "                plt.figure()  # Cria uma nova figura para cada método\n",
    "                plt.scatter(data_preprocessed[:, 0], data_preprocessed[:, 1], c=labels_pred)  # Plotar dados com cores de cluster\n",
    "                plt.title(f\"{method_name} on {dataset_name}\")\n",
    "                plt.xlabel(\"Component 1\")\n",
    "                plt.ylabel(\"Component 2\")\n",
    "                plt.show()  # Exibe o gráfico na tela\n",
    "                plt.savefig(f\"figs/{dataset_name}_{method_name}.png\")\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "\"\"\"Para o Silhouette Score e Davies-Bouldin Score, quanto maior o valor, melhor é o agrupamento.\n",
    "Para o ARI e NMI, quanto mais próximo de 1, melhor é a concordância entre os agrupamentos e os rótulos verdadeiros.\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
